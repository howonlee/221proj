\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.0in]{geometry}

\usepackage{hyperref}

\begin{document}
\title{project progress}
\author{howon lee}
\maketitle
\section*{Note recommendation}

\subsection*{Task Definition}
The system will take in computer keyboard input and play music as if it were a MIDI keyboard. This by itself is not too terribly impressive, but what it should also do is to give suggestions in real-time for what notes to play, for the player who wants a little bit of help improvising.

There are two ways of looking at the task there: perhaps it is giving suggestions in real-time for what notes to play based upon a corpus of stored MIDI music data, in which case the actual task would be time series generation. Or perhaps it is being rewarded or punished by the person at the keyboard playing its notes, in which case it would be doing reinforcement learning.

In either case, it seems to me that the best measurement for the success of the system will be the percentage of time that the person at the keyboard plays notes in accordance with the suggestions. If we look at the task as a time series generation, that ends up being the percentage of time that the person at the keyboard is in accordance with the generated time series. Of course there has to also be a large qualitative component in the actual aesthetics of what such a system generates, but this is what we can put on a chart. 

\subsection*{Hypothesis}
It seems to be the case that the closest analogy to this task is word-completion or word-modelling with a small vocabulary. This is because the musical notes can be seen to be the characters of words, or musical phrases seen to be words entire. This task also seems amenable to inclusion under the reinforcement learning paradigm, because it seems that the actual behavior of the player is important as a feedback mechanism. Therefore, there should be two hypotheses which can
be tested:

\begin{enumerate}
    \item It is the case that the models that people use for language modeling will work for this task.
    \item It is the case that reinforcment learning is a good AI paradigm for the task.
\end{enumerate}

To test the full assumptions behind the first hypothesis is too big a task, so what I will actually investigate for the first task is whether the Kneser-Ney smoothing give us better performance for this task than other smoothing methods, just as it does compared to other smoothing methods for language models.

The baseline here in both cases will be the unsmoothed HMM taught using EM. It seems that the underlying state would be the chord from which each pitch is taken, so that is what I am using for the hidden variables.

By "Kneser-Ney smoothing", I mean the interpolated Kneser-Ney smoothing. This is the model which combines both the higher-order and lower-order distributions, like so:

$$P_{IKN}(w_i | w_{i-1}) = \frac{C(w_{i-1}w_i) - D}{C(w_{i-1})} + \lambda (w_{i-1}) \frac{|\{v | C(vw_i) > 0 \}|}{\sum_w |\{v|C(vw) > 0\}|}$$

Where $\lambda(w_{i-1})$ is a normalization constant, C is the count of the n-gram, D is a single discount optimized on held-out data, and $|\{v|C(vw_i) > 0\}|$ is the number of words that $v$ and $w_i$ can occur in the context of. See the Kneser-Ney and Goodman papers for details.

This I will compare with no smoothing, additive smoothing, and Katz smoothing.

By "Q-learning", I mean the model-free reinforcement learning technique. It seems tenable to back Q-learning with a table, since the state space for this model will not be complicated.

\subsection*{Baseline System}
I have implemented the keyboard music part of the system. I will be using the pre-cleaned piano-roll MIDI files from the LISA lab found here:

\url{http://www-etud.iro.umontreal.ca/~boulanni/icml2012}

Previously, I thought that Naive Bayes would do as a baseline, but the performance was so terrible (the classifier only predicted that the next note would be C, F or G all the time), that I will switch the baseline. For a baseline, I set up a HMM model learned by EM to generate the best note prediction.

\subsection*{Further Development}
In comparing language models, I am doing a simple 3-gram feature of the note pitch values, taking the pre-cleaned data, which is always transposed to C major. The way that I am testing the assumption that Kneser-Ney smoothing gives us better performance is by making a HMM model and comparing the performance of a variety of smoothing methods with the modified Kneser-Ney smoothing. It seems a settled empirical result that modified Kneser-Ney smoothing is a smoothing method
which is better than the other smoothing models I am comparing it to (additive, etc) for language models, but that may not be the case for this task.

The way that I am testing the assumption that reinforcement learning gives us better performance is by comparing an HMM model with a Q-learning model and seeing their respective performances. I haven't gotten to this part yet, but I have more features on the data. 

In addition to an unsmoothed 3-gram feature of note pitch values, I also have note pitch differentials (the difference between a note and the note before it), the and the variance statistic on pitch.

\begin{thebibliography}{9}%no more than 9 references

\bibitem{KneserNey}
Ney, H., et al., On Structuring Probabilistic Dependences in Stochastic Language Modeling, in Computer, Speech and Language, 1994

\bibitem{Goodman}
Goodman, J., A Bit of Progress in Language Modeling, in Computer Speech & Language, 2001

\bibitem{JSymbolic}
McKay, C., and Fujinaga, I., jSymbolic: A Feature Extractor for MIDI Files, in Proceedings of the International Computer Music Conference, 2006

\bibitem{BoulangerLewandowski12}
N Boulanger-Lewandowsky, Y. Bengio and P. Vincent, Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription, in Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.

\bibitem{Rabiner}
L. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, in Proceeding of the IEEE, 1989.

\bibitem{Watkins}
C. Watkins, Technical Note on Q-Learning, in Machine Learning, 1992.

\bibitem{Rummery}
N. Taghipour, A. Kardan, A Hybrid Web Recommender System Based on Q-Learning, in the 8th ACM Sympsium on Applied Computing, 2008

\end{thebibliography}

\end{document}
