\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.5in]{geometry}

\usepackage{hyperref}

\begin{document}
\title{CS221 Project}
\author{Howon Lee}
\maketitle
\section*{Real-Time Note Recommendation}

\subsection*{Problem Definition}
Given previous set of notes, predict a next tone within 12 possible tones in real time

\subsection*{Challenges And Unproductive Approaches}
Actually quite hard
RBM didn't work so well, too slow
Non-laplace smoothing didn't even make a qualitative difference

\subsection*{Hypothesis}

Testable hypotheses
\begin{enumerate}
    \item The predictor will do better than random
    \item The predictor will also do better than prediction from non-appropriate data (JS Bach)
    \item The model matters in prediction task (qualitative, quantitative results)
\end{enumerate}

\subsection*{Methods}
Training dataset: stored notes from a piano player playing
Test dataset: the same piano player playing something different
The piano player could not see the predictions that the system makes in either case, to allow the test set to work

Markov Model: states are notes, to be used as baseline. I also did a higher-order Markov Model.

\subsubsection*{Hidden Markove Model State}
Observed states are notes in HMM.
Hidden states were tried:
1. Previous note
2. First note of 4-note run
3. Difference between previous note

\subsubsection*{Reinforcement Learning State}
1. Previous note
2. Difference between previous note

\subsection*{Results and Analysis}
Hypothesis I sustained
Hypothesis II not sustained for quantative (accuracy measure on prediction), but qualitative (which predictions they did make), did make a difference

\subsection*{Conclusion}
A predictor for real-time note recommendation was made. Although it was not incredible accurate, it was better than random. A variety of models were considered. These different models did not have a quantitative difference but they did have a qualitative difference on the predictions made.

Try the program out to see the qualitative difference.

\subsection*{Acknowledgement}
Thanks to J. Li, who made the training and test data by playing the keyboard program.

\begin{thebibliography}{9}%no more than 9 references

\bibitem{KneserNey}
Ney, H., et al., On Structuring Probabilistic Dependences in Stochastic Language Modeling, in Computer, Speech and Language, 1994

\bibitem{Goodman}
Goodman, J., A Bit of Progress in Language Modeling, in Computer Speech and Language, 2001

\bibitem{JSymbolic}
McKay, C., and Fujinaga, I., jSymbolic: A Feature Extractor for MIDI Files, in Proceedings of the International Computer Music Conference, 2006

\bibitem{Katz}
Katz, S. M., Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, in IEEE Transactions on Acoustics, Speech and Signal Processing, 1987

\bibitem{BoulangerLewandowski12}
N Boulanger-Lewandowsky, Y. Bengio and P. Vincent, Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription, in Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.

\bibitem{Rabiner}
L. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, in Proceeding of the IEEE, 1989.

\bibitem{Watkins}
C. Watkins, Technical Note on Q-Learning, in Machine Learning, 1992.

\bibitem{Rummery}
N. Taghipour, A. Kardan, A Hybrid Web Recommender System Based on Q-Learning, in the 8th ACM Sympsium on Applied Computing, 2008

\bibitem{Schulze}
Schulze, W., and van der Merwe, B., Music Generation with Markov Models, in MultiMedia, IEEE, 2011

\end{thebibliography}

\end{document}
