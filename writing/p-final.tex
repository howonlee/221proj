\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.5in]{geometry}

\usepackage{hyperref}

\begin{document}
\title{CS221 Project}
\author{Howon Lee}
\maketitle
\section*{Real-Time Note Recommendation}

\subsection*{Motivation}
\subsection*{Problem Definition}
The system will take in computer keyboard input and play music as if it were a MIDI keyboard. This by itself is not too terribly impressive, but what it should also do is to give suggestions in real-time for what notes to play, for the player who wants a little bit of help improvising.

There are two ways of looking at the task there: perhaps it is giving suggestions in real-time for what notes to play based upon a corpus of stored MIDI music data, in which case the actual task would be time series generation. Or perhaps it is being rewarded or punished by the person at the keyboard playing its notes, in which case it would be doing reinforcement learning.

In either case, it seems to me that the best measurement for the success of the system will be the percentage of time that the person at the keyboard plays notes in accordance with the suggestions. If we look at the task as a time series generation, that ends up being the percentage of time that the person at the keyboard is in accordance with the generated time series. All that needs to be done would be to hide the recommendations when we want to evaluate the system and show them when we want to \emph{use} the system. Of course, there has to also be a large qualitative component in the actual aesthetics of what such a system generates, but this is what we can put on a chart. 

\subsection*{Challenges}

\subsection*{Hypothesis}
It seems to be the case that the closest analogy to this task is word-completion or word-modelling with a small vocabulary. This is because the musical notes can be seen to be the characters of words, or musical phrases seen to be words entire. This task also seems amenable to inclusion under the reinforcement learning paradigm, because it seems that the actual behavior of the player is important as a feedback mechanism. Therefore, there should be two hypotheses which can be tested:

\begin{enumerate}
    \item It is the case that the models that people use for language modeling will work for this task.
    \item It is the case that reinforcment learning is a good AI paradigm for the task.
\end{enumerate}

To test the full assumptions behind the first hypothesis is too big a task. What I will actually investigate for the first task for this project can be two things:

\begin{enumerate}
    \item Whether the Kneser-Ney smoothing give us better performance for this task than other smoothing methods, just as it does compared to other smoothing methods for language models.
    \item Whether we can make the generation of pitches better by incorporating hidden state.
\end{enumerate}

So three statements to evaluate, in all.

The baseline here in both cases will be the unsmoothed 3-gram Markov model. 

\subsection*{Approaches}

\subsubsection*{Hidden State}
The sequence of pitches is a time series, and has state in that there is a history of pitches. It also seems to be the case that the underlying process which is generating the pitches might have structure, even though we don't see it. Therefore, it might be that there is hidden state. In testing the idea that our performance would be improved by hidden state, it seems that we need to prescribe an underlying state\cite{Rabiner}. It seems that the underlying state would be the chord from which each pitch is taken. That is, the hidden state in the HMM would be what chord that we're currently in, and each chord would have emission probabilities for emitting the pitch that we are currently on, trained by EM. There are many other, better ways to do this, but this is a very simple one that has been previously tried in the literature\cite{Schulze}.

\subsubsection*{Reinforcement Learning}

It seems tenable to back Q-learning with a table, since the state space for this model will not be complicated. The reward for the Q-learning will be whether or not the player actually takes the recommendation of a pitch. The possible states for the MDP for the algorithm is the pitches, and the actions are productions from pitch to pitch. Therefore, the policy can be learned by Q-learning.

\subsection*{Baseline System}
I have implemented the keyboard music part of the system. I will be using the pre-cleaned piano-roll MIDI files from the LISA lab found here:

\url{http://www-etud.iro.umontreal.ca/~boulanni/icml2012}

Previously, I thought that Naive Bayes would do as a baseline, but the performance was so terrible (the classifier only predicted that the next note would be C, F or G all the time), that I switched the baseline.

\subsection*{Results and Analysis}

\begin{thebibliography}{9}%no more than 9 references

\bibitem{KneserNey}
Ney, H., et al., On Structuring Probabilistic Dependences in Stochastic Language Modeling, in Computer, Speech and Language, 1994

\bibitem{Goodman}
Goodman, J., A Bit of Progress in Language Modeling, in Computer Speech and Language, 2001

\bibitem{JSymbolic}
McKay, C., and Fujinaga, I., jSymbolic: A Feature Extractor for MIDI Files, in Proceedings of the International Computer Music Conference, 2006

\bibitem{Katz}
Katz, S. M., Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, in IEEE Transactions on Acoustics, Speech and Signal Processing, 1987

\bibitem{BoulangerLewandowski12}
N Boulanger-Lewandowsky, Y. Bengio and P. Vincent, Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription, in Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.

\bibitem{Rabiner}
L. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, in Proceeding of the IEEE, 1989.

\bibitem{Watkins}
C. Watkins, Technical Note on Q-Learning, in Machine Learning, 1992.

\bibitem{Rummery}
N. Taghipour, A. Kardan, A Hybrid Web Recommender System Based on Q-Learning, in the 8th ACM Sympsium on Applied Computing, 2008

\bibitem{Schulze}
Schulze, W., and van der Merwe, B., Music Generation with Markov Models, in MultiMedia, IEEE, 2011

\end{thebibliography}

\end{document}
