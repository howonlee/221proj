\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.5in]{geometry}

\usepackage{hyperref}

\begin{document}
\title{CS221 Project: Real-Time Note Recommendation}
\author{Howon Lee}
\maketitle
\subsection*{Problem Definition}
%%%%pretty picture here
A system was made to try and predict a good next pitch to play within 12 possible tones in real time, given a training set of notes. Basically, a probability distribution was made out of a training set of notes and we tried to predict such that, if a person took the "predicted" pitches, they would play from a similar probability distribution.
The data had to be gathered from a real piano player playing, so I got a piano player to create the training data and test out the system.

%%%%lit review actually goes here, I guess

\subsection*{Challenges And Unproductive Approaches}
This is definitely something that must run in real-time, since ideally, the system would be used in real-time.
As a task in itself, the task needs domain expertise, because the representation of music has to be transformed into something tractable.
Non-laplace smoothing didn't make a qualitative or quantitative difference, even thought I thought it would.

\subsection*{Hypotheses}
These are the hypotheses about the system that I made with reference to the problem statement and that I could test with experiments.
\begin{enumerate}
    \item The predictor will do better than random
    \item The predictor will also do better than prediction from non-appropriate data (JS Bach)
    \item The model matters in prediction task (qualitative, quantitative results)
\end{enumerate}
So I have to test the predictor versus random, the predictor versus prediction from a non-appropriate dataset, and I have to compare the models in the prediction task in two ways: qualitative and quantitative.

\subsection*{Methods}
\subsubsection*{Training Dataset}
The training dataset was stored notes from a piano player playing and notes from the music of JS Bach, both in MIDI pitch format. If the predictor trained on stored notes would do better than random on the test set, then that would satisfy the first hypothesis, and if the predictor trained on stored notes would do better than the predictor trained on JS Bach, that would satisfy the second hypothesis.
\subsubsection*{Test Dataset}
The test dataset was stored notes from the same piano player playing at a different time. The piano player could not see the predictions that the system makes in either case, because the test set would not be a very good measure of the goodness of the prediction that way.

Since the core task of the system seems to be predicting the pitches themselves, irrespective to which octave they were in, the predictions are measured by whether they are the correct pitch, but irrespective of octave: that is, if we were supposed to get an F4 (according to the piano player's next note), and we actually got an F5, that was counted as correct.
\subsubsection*{Models}

Probability distributions were Laplace smoothed with $\alpha = 0.05$ before normalization.

For a baseline, a Markov model (2-gram) was used, along with another, higher-order Markov model (3-gram). These were trained and made prediction based on all the pitches (so F5 considered different than F4). This is a good baseline, since the pitch prediction is fundamentally a time series task. A semimarkov assumption is not unwarranted as a first approximation, if we accept that there are certain short intervals that are much more probable than other ones.

\subsubsection*{Hidden Markov Model}
It seems to be justifiable to say that there might be separate hidden and observable components in the music, since the same E might be played at different times for different reasons. Therefore, a hidden markov model was tried.

The observed states were considered the note pitches in the HMM model, since the interaction the player had with the system was by playing the notes.
Different hidden states were tried:
%%%%REEEEEAAAASSONS
1. It might be that the previous pitch played generates the observed note, so the previous pitch played could have been the states (this is different from the Markov model because of the 12-state space and the fact that I wasn't sampling but getting the maximum expectation of the distribution).
2. It might also be that a lot of four-note runs are produced by the certain class of these short sequences, so the first note of 4-note run, was tried as the hidden state configuration.
3. Note that a tritone interval sounds much the same anywhere, so it might have been the case that the next note is generated from the difference between the previous note and the note before that.

Forward-backwards training was used. Predictions were made by Viterbi algorithm. Because I limited the state space, the HMM was fast enough. %%%%GRAPHSSSSSS of memory

\subsubsection*{Reinforcement Learning}
It seems to be the case that there's a clearly defined conception of reward, as defined as the piano player actually hitting the predicted pitch. Therefore, Q-learning was tried.
Q-learning was done with epsilon-greedy method. Different states configurations were tried:
%%%%REEEEEAAAASSONS
1. The previous note could be construed as a state from which actions, construed as notees, were generated.
2. The difference between previous note could also be construed as a state from which actions, construed as notes, were generated.
We did not try first note of 4-note run because it seemed that this corresponded to a hidden state only, whereas it was plausible that the previous note and difference between previous note also corresponded to an observable state.

\subsection*{Results and Analysis}
%%%%% GRAPHHHHHS
Note that the statistical tests to verify or falsify the null hypothesis with respect to distributions was the Kolmogorov-Smirnov test, so if I say "significant", I mean, "significant with respect to the KS test $(p<0.05)$".

Hypothesis I should be sustained, because there was a significant difference between the accuracy and average F1 scores (by class) of the model predictions and the JSB data. There is also a significant difference between the accuracy and average F1 scores of the model predictions and what would be predicted for random predictions drawn from a uniform probability distribution over the 12 possible tones.
Hypothesis II not sustained for quantitative (accuracy measure on prediction), because there was not a significant difference between the accuracy or the average F1 of the models, again construed as a time series.
However, qualitative (which predictions they did make), were significantly different, not only among the models but in between each of the state configurations of all the models. Here, once can also tell this by visual inspection of the confusion matrices of some of the models. To wit, the models got most things wrong, but it got those things wrong in different ways. For a concrete example, look at the confusion matrices below, and the different pattern with which the Q-learner predicts notes versus the higher-order non-hidden Markov model.
The reason why this happened seemed to be that the HMM still had the Markov property, and Q-Learning did not go on for long enough to satisfactorily explore the pitch state space in a way that did better than the other models. It may be worth later exploring structured models like PCFG's, which are better at creating structured predictions, since music is often structured like a context-free grammar.
%%%CONF MAT PICCCCCCS

\subsection*{Conclusion}
A predictor for real-time note recommendation was made.
Although it was not incredibly accurate, it was better than random, and it does seem to represent a probability distribution.
A variety of models were considered, with state configurations from a little bit of domain knowledge.
These different models did not have a quantitative difference but they did have a qualitative difference on the predictions made, and they were different from one another.

Try the program out to see the qualitative difference, if you wish.

\subsection*{Acknowledgement}
Thanks to J. Li, the piano player, and the CS221 teaching staff for much-appreciated help and advice.

\begin{thebibliography}{9}

\bibitem{KneserNey}
Ney, H., et al., On Structuring Probabilistic Dependences in Stochastic Language Modeling, in Computer, Speech and Language, 1994

\bibitem{Goodman}
Goodman, J., A Bit of Progress in Language Modeling, in Computer Speech and Language, 2001

\bibitem{JSymbolic}
McKay, C., and Fujinaga, I., jSymbolic: A Feature Extractor for MIDI Files, in Proceedings of the International Computer Music Conference, 2006

\bibitem{Katz}
Katz, S. M., Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, in IEEE Transactions on Acoustics, Speech and Signal Processing, 1987

\bibitem{BoulangerLewandowski12}
N Boulanger-Lewandowsky, Y. Bengio and P. Vincent, Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription, in Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.

\bibitem{Rabiner}
L. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, in Proceeding of the IEEE, 1989.

\bibitem{Watkins}
C. Watkins, Technical Note on Q-Learning, in Machine Learning, 1992.

\bibitem{Rummery}
N. Taghipour, A. Kardan, A Hybrid Web Recommender System Based on Q-Learning, in the 8th ACM Sympsium on Applied Computing, 2008

\bibitem{Schulze}
Schulze, W., and van der Merwe, B., Music Generation with Markov Models, in MultiMedia, IEEE, 2011

\end{thebibliography}

\end{document}
